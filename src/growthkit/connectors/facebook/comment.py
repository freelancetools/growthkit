"""Extract comments from Facebook ads.

Workflow:
1. Read Ad IDs (either from a file like config/facebook/ad-ids.txt or via the --ids CLI argument).
2. Use Marketing API to resolve each Ad's creative → effective_object_story_id → Post ID.
3. For each Post ID, fetch all comments (with pagination).
4. Persist everything to data/facebook/comments-YYYYMMDD-HHMMSS.json.

Requirements
------------
* A long-lived **user access token** that has the `ads_read` permission (used for Ad→Post lookup).
  We load this from the latest tokens-*.json file generated by `growthkit.connectors.facebook.tokens`.
* Page access tokens (generated by the same tool) for comment lookup. If a post's Page ID
  isn't found in the token file, we fall back to the user token - this works as long as the 
  user token has `pages_read_user_content`.

Example
-------
$ python -m growthkit.connectors.facebook.comment \
    --ids-file config/facebook/ad-ids.txt \
    --output data/facebook
"""

import json
import sys
import argparse
import urllib.parse
import urllib.request
import time
from pathlib import Path
from datetime import datetime
from typing import Dict, Any, List, Tuple, Optional

from growthkit.utils.style import ansi
from growthkit.utils.logs import report
from growthkit.connectors.facebook.engine import TokenManager

logger = report.settings(__file__)

# Constants
CONFIG = Path("config", "facebook")
TOKENS_DIR = Path(CONFIG, "tokens")
DATA_FACEBOOK = Path("data", "facebook")
DEFAULT_IDS_FILE = Path(CONFIG, "ad-ids.txt")  # moved to config
GRAPH_VERSION = "v23.0"
BASE_URL = f"https://graph.facebook.com/{GRAPH_VERSION}"
CHUNK_SIZE = 50  # Graph API lets you batch ~50 IDs per request

# HTTP settings
REQUEST_TIMEOUT = 30  # 30 seconds timeout
MAX_RETRIES = 3
RETRY_DELAY = 2  # seconds between retries
RATE_LIMIT_DELAY = 1  # seconds between requests to avoid rate limiting

#############################
# Utility helpers           #
#############################

def load_latest_tokens() -> Tuple[str, Dict[str, str]]:
    """Return (user_access_token, {page_id: page_token})."""
    logger.info("Starting token loading process")
    tm = TokenManager()
    latest = tm.get_latest_run_file()

    if latest is None:
        logger.error("No token files found in tokens directory")
        error_msg = "No token files found. Run `growthkit.connectors.facebook.tokens` first."
        print(f"{ansi.red}{error_msg}{ansi.reset}")
        sys.exit(1)

    logger.info("Loading token data from file: %s", latest)
    tm.load_run_data(latest)

    long_lived_token = tm.user_config.long_lived_token
    user_token = long_lived_token.access_token if long_lived_token else None

    if not user_token:
        logger.error("User long-lived token missing from token file: %s", latest)
        print(f"{ansi.red}User long-lived token missing in {latest}{ansi.reset}")
        sys.exit(1)

    logger.info("Successfully loaded user access token (length: %d chars)", len(user_token))

    page_tokens = {}
    for page_id, cfg in tm.page_configs.items():
        if cfg.page_access_token and cfg.page_access_token.access_token:
            page_tokens[page_id] = cfg.page_access_token.access_token
            logger.debug("Loaded page token for page_id: %s (length: %d chars)",
                        page_id, len(cfg.page_access_token.access_token))

    logger.info("Token loading complete - user token: available, page tokens: %d pages",
               len(page_tokens))
    return user_token, page_tokens


def graph_request(endpoint: str, params: Dict[str, Any], retry_count: int = 0) -> Dict[str, Any]:
    """Make a Graph API request with error handling, timeout, and retry logic."""

    request_start = time.perf_counter()
    query = urllib.parse.urlencode(params)
    url = f"{BASE_URL}/{endpoint}?{query}"

    # Log request details (sanitize access token)
    sanitized_params = {
        k: (v[:20] + "..." if k == "access_token" and len(str(v)) > 20 else v)
        for k, v in params.items()
    }
    logger.info("Making Graph API request to endpoint: %s", endpoint)
    logger.debug("Request parameters: %s", sanitized_params)
    logger.debug("Full URL (first 200 chars): %s", url[:200])

    if retry_count > 0:
        logger.warning("Retry attempt %d/%d for endpoint: %s",
                      retry_count, MAX_RETRIES, endpoint)

    logger.info("Graph GET %s", url[:120] + ("…" if len(url) > 120 else ""))

    print(f"\n{ansi.blue}DEBUG: Making Graph API request:{ansi.reset}")
    print(f"  URL: {url[:150]}{'...' if len(url) > 150 else ''}")
    if retry_count > 0:
        print(f"  Retry attempt: {retry_count}/{MAX_RETRIES}")

    try:
        req = urllib.request.Request(url)
        with urllib.request.urlopen(req, timeout=REQUEST_TIMEOUT) as resp:
            response_time = time.perf_counter() - request_start
            body = resp.read().decode()

            logger.info("Graph API response received in %.3f seconds", response_time)
            logger.info("Response status: %d, Content-Length: %d bytes",
                       resp.status, len(body))

            print(f"  Response status: {ansi.green}{resp.status}{ansi.reset}")
            print(f"  Response body length: {len(body)} characters")
            body_preview = f"{body[:500]}{'...' if len(body) > 500 else ''}"
            print(f"  Response body (first 500 chars): {ansi.grey}{body_preview}{ansi.reset}")

            data = json.loads(body)
            logger.debug("Response JSON parsed successfully, top-level keys: %s",
                        list(data.keys()))

            # Check for Facebook errors in the response
            if 'error' in data:
                error = data['error']
                logger.error("Facebook API error - Code: %s, Type: %s, Message: %s",
                            error.get('code', 'Unknown'),
                            error.get('type', 'Unknown'),
                            error.get('message', 'Unknown'))

                print(f"  {ansi.red}Facebook API Error:{ansi.reset}")
                print(f"    Message: {error.get('message', 'Unknown')}")
                print(f"    Type: {error.get('type', 'Unknown')}")
                print(f"    Code: {error.get('code', 'Unknown')}")

                # Handle rate limiting specifically
                if error.get('code') in [4, 17, 32]:  # Rate limit error codes
                    logger.warning("Rate limit detected (code: %s), retry count: %d",
                                  error.get('code'), retry_count)
                    if retry_count < MAX_RETRIES:
                        delay = RETRY_DELAY * (2 ** retry_count)  # Exponential backoff
                        logger.info("Waiting %d seconds before retry %d/%d",
                                   delay, retry_count + 1, MAX_RETRIES)
                        print(f"  {ansi.yellow}Rate limit detected, waiting {delay}s "
                              f"before retry...{ansi.reset}")
                        time.sleep(delay)
                        return graph_request(endpoint, params, retry_count + 1)

                    logger.error("Rate limit retry exhausted after %d attempts", MAX_RETRIES)
                    raise RuntimeError(f"Rate limit exceeded after {MAX_RETRIES} retries")

            logger.info("Graph API request completed successfully in %.3f seconds", response_time)
            return data

    except urllib.error.HTTPError as e:
        response_time = time.perf_counter() - request_start
        err_body = e.read().decode(errors="ignore") if hasattr(e, "read") else ""

        logger.error("HTTP Error %d (%s) after %.3f seconds for endpoint: %s",
                    e.code, e.reason, response_time, endpoint)
        logger.debug("HTTP Error body: %s", err_body[:500])

        print(f"  {ansi.red}HTTP Error {e.code}: {e.reason}{ansi.reset}")
        error_preview = f"{err_body[:300]}{'...' if len(err_body) > 300 else ''}"
        print(f"  Error body: {ansi.red}{error_preview}{ansi.reset}")

        # Retry on server errors (5xx) or rate limits (429)
        if e.code in [429, 500, 502, 503, 504] and retry_count < MAX_RETRIES:
            delay = RETRY_DELAY * (2 ** retry_count)
            logger.warning("Retrying HTTP %d error in %d seconds (attempt %d/%d) for endpoint: %s",
                          e.code, delay, retry_count + 1, MAX_RETRIES, endpoint)
            print(f"  {ansi.yellow}Retrying in {delay}s... "
                  f"(attempt {retry_count + 1}/{MAX_RETRIES}){ansi.reset}")
            time.sleep(delay)
            return graph_request(endpoint, params, retry_count + 1)

        logger.error("HTTP %s: %s – %s", e.code, e.reason, err_body[:200])
        raise

    except (urllib.error.URLError, OSError, ConnectionError) as e:
        response_time = time.perf_counter() - request_start
        logger.error("Network error after %.3f seconds for endpoint %s: %s",
                    response_time, endpoint, str(e))

        print(f"  {ansi.red}Network Error: {str(e)}{ansi.reset}")

        # Retry on network errors
        if retry_count < MAX_RETRIES:
            delay = RETRY_DELAY * (2 ** retry_count)
            logger.warning("Retrying network error in %d seconds (attempt %d/%d) for endpoint: %s",
                          delay, retry_count + 1, MAX_RETRIES, endpoint)
            print(f"  {ansi.yellow}Network error, retrying in {delay}s... "
                  f"(attempt {retry_count + 1}/{MAX_RETRIES}){ansi.reset}")
            time.sleep(delay)
            return graph_request(endpoint, params, retry_count + 1)

        logger.error("Network error after %d retries: %s", MAX_RETRIES, str(e))
        raise


def chunked(lst: List[str], size: int) -> List[List[str]]:
    """Split a list into chunks of specified size."""
    for i in range(0, len(lst), size):
        yield lst[i:i + size]

#############################
# Core logic                #
#############################

def ad_ids_to_post_ids(ad_ids: List[str], user_token: str) -> Dict[str, List[str]]:
    """Return mapping {ad_id: [post_id1, post_id2, ...]}.
    
    Returns all posts associated with each ad.
    """
    function_start = time.perf_counter()

    logger.info("Starting ad_ids_to_post_ids function with %d ads", len(ad_ids))
    logger.debug("Input ad_ids (first 10): %s", ad_ids[:10])
    logger.debug("User token length: %d characters", len(user_token))

    mapping: Dict[str, List[str]] = {}

    print(f"\n{ansi.cyan}DEBUG: Starting ad-to-post resolution for {len(ad_ids)} ads{ansi.reset}")
    print(f"{ansi.cyan}DEBUG: Will fetch ALL creatives per ad to capture multiple posts{ansi.reset}")

    chunks = list(chunked(ad_ids, CHUNK_SIZE))
    logger.info("Split %d ads into %d chunks (chunk_size=%d)",
               len(ad_ids), len(chunks), CHUNK_SIZE)

    for chunk_idx, chunk in enumerate(chunks):
        # Add rate limiting delay between chunks (except for the first one)
        chunk_start = time.perf_counter()
        logger.info("Processing chunk %d/%d with %d ads",
                   chunk_idx + 1, len(chunks), len(chunk))
        logger.debug("Chunk %d ad_ids: %s", chunk_idx + 1, chunk)

        if chunk_idx > 0:
            logger.info("Rate limiting: waiting %d seconds before chunk %d",
                       RATE_LIMIT_DELAY, chunk_idx + 1)
            print(f"\n{ansi.yellow}Rate limiting: waiting {RATE_LIMIT_DELAY}s "
                  f"before processing next chunk...{ansi.reset}")
            time.sleep(RATE_LIMIT_DELAY)

        ids_str = ",".join(chunk)
        # Request ALL creatives for each ad, not just the primary one
        params = {
            "ids": ids_str,
            "fields": "adcreatives{effective_object_story_id,object_story_id,object_id,name}",
            "access_token": user_token,
        }

        logger.debug("Chunk %d API parameters: fields=%s, ads_count=%d",
                    chunk_idx + 1, params['fields'], len(chunk))

        print(f"\n{ansi.yellow}DEBUG: Requesting chunk {chunk_idx + 1}/{len(chunks)} "
              f"with {len(chunk)} ads:{ansi.reset}")
        for i, ad_id in enumerate(chunk, 1):
            print(f"  {i}. Ad ID: {ansi.cyan}{ad_id}{ansi.reset}")

        print(f"{ansi.yellow}DEBUG: API call params:{ansi.reset}")
        print(f"  fields: {params['fields']}")
        print(f"  ids: {ids_str}")

        try:
            logger.info("Making Graph API request for chunk %d with %d ads",
                       chunk_idx + 1, len(chunk))
            data = graph_request("", params)  # blank endpoint when using ids param

            chunk_response_time = time.perf_counter() - chunk_start
            logger.info("Chunk %d API response received in %.3f seconds",
                       chunk_idx + 1, chunk_response_time)
            logger.debug("Chunk %d response contains %d ads", chunk_idx + 1, len(data))

            print(f"\n{ansi.green}DEBUG: API Response received:{ansi.reset}")
            print(f"  Response keys: {list(data.keys())}")

            for ad_id, ad_data in data.items():
                logger.debug("Processing ad_id: %s", ad_id)
                logger.debug("Raw ad_data for %s: %s", ad_id, json.dumps(ad_data, indent=2)[:500])

                print(f"\n{ansi.cyan}Processing Ad ID: {ad_id}{ansi.reset}")
                print(f"  Raw ad_data: {json.dumps(ad_data, indent=2)}")

                # Get all creatives for this ad
                adcreatives = ad_data.get("adcreatives", {})
                creatives_data = adcreatives.get("data", [])

                logger.info("Ad %s has %d creatives", ad_id, len(creatives_data))
                print(f"  Found {ansi.yellow}{len(creatives_data)}{ansi.reset} "
                      f"creatives for this ad")

                post_ids = []
                for creative_idx, creative in enumerate(creatives_data, 1):
                    logger.debug("Ad %s creative %d: %s", ad_id, creative_idx, json.dumps(creative))

                    print(f"  \n  {ansi.magenta}Creative {creative_idx}:{ansi.reset}")
                    print(f"    Creative data: {json.dumps(creative, indent=4)}")

                    # Try multiple fields that might contain post IDs
                    potential_post_id = None

                    # Priority order: effective_object_story_id > object_story_id > object_id
                    if creative.get("effective_object_story_id"):
                        potential_post_id = creative["effective_object_story_id"]
                        logger.debug("Ad %s creative %d found effective_object_story_id: %s",
                                    ad_id, creative_idx, potential_post_id)
                        print(f"    Found effective_object_story_id: "
                              f"{ansi.yellow}{potential_post_id}{ansi.reset}")
                    elif creative.get("object_story_id"):
                        potential_post_id = creative["object_story_id"]
                        logger.debug("Ad %s creative %d found object_story_id: %s",
                                    ad_id, creative_idx, potential_post_id)
                        print(f"    Found object_story_id: "
                              f"{ansi.yellow}{potential_post_id}{ansi.reset}")
                    elif creative.get("object_id"):
                        potential_post_id = creative["object_id"]
                        logger.debug("Ad %s creative %d found object_id: %s",
                                    ad_id, creative_idx, potential_post_id)
                        print(f"    Found object_id: "
                              f"{ansi.yellow}{potential_post_id}{ansi.reset}")
                    else:
                        logger.warning("Ad %s creative %d has no post ID fields. Available fields: %s",
                                      ad_id, creative_idx, list(creative.keys()))
                        print(f"    {ansi.red}No post ID found in this creative{ansi.reset}")
                        # Debug: show all available fields
                        print(f"    Available fields: {list(creative.keys())}")

                    if potential_post_id and potential_post_id not in post_ids:
                        post_ids.append(potential_post_id)
                        logger.debug("Ad %s: Added unique post_id %s (total: %d)",
                                    ad_id, potential_post_id, len(post_ids))
                        print(f"    {ansi.green}✓ Added to post list{ansi.reset}: {potential_post_id}")
                    elif potential_post_id:
                        logger.debug("Ad %s: Skipping duplicate post_id %s", ad_id, potential_post_id)
                        print(f"    {ansi.yellow}⚠ Duplicate post ID, skipping{ansi.reset}: "
                              f"{potential_post_id}")

                if post_ids:
                    mapping[ad_id] = post_ids
                    logger.info("Ad %s successfully mapped to %d posts: %s",
                               ad_id, len(post_ids), post_ids)
                    print(f"  {ansi.green}✓ Successfully mapped{ansi.reset}: "
                          f"{ad_id} → {post_ids} ({len(post_ids)} posts)")
                else:
                    mapping[ad_id] = []
                    logger.warning("Ad %s – no post IDs found in any creatives", ad_id)
                    print(f"  {ansi.red}✗ No post IDs found for this ad{ansi.reset}")

        except Exception as e:
            chunk_error_time = time.perf_counter() - chunk_start
            logger.error("Chunk %d failed after %.3f seconds: %s",
                        chunk_idx + 1, chunk_error_time, str(e))
            logger.error("Failed chunk contained ad_ids: %s", chunk)

            print(f"\n{ansi.red}DEBUG: API request failed:{ansi.reset}")
            print(f"  Error: {str(e)}")
            print(f"  Ad IDs in this chunk: {chunk}")
            raise

    # Calculate totals and function completion
    function_time = time.perf_counter() - function_start
    total_posts = sum(len(posts) for posts in mapping.values())
    ads_with_multiple_posts = sum(1 for posts in mapping.values() if len(posts) > 1)
    ads_with_no_posts = len([posts for posts in mapping.values() if len(posts) == 0])

    logger.info("ad_ids_to_post_ids completed in %.3f seconds", function_time)
    logger.info("Processing summary: %d ads → %d total posts (%d unique posts found)",
               len(ad_ids), total_posts,
               len(set(post for posts in mapping.values() for post in posts)))
    logger.info("Ad distribution: %d ads with multiple posts, %d ads with no posts",
               ads_with_multiple_posts, ads_with_no_posts)

    print(f"\n{ansi.cyan}DEBUG: Ad-to-post resolution complete:{ansi.reset}")
    print(f"  Total ads processed: {len(ad_ids)}")
    print(f"  Total unique posts found: {ansi.yellow}{total_posts}{ansi.reset}")
    print(f"  Ads with multiple posts: {ansi.yellow}{ads_with_multiple_posts}{ansi.reset}")
    print(f"  Ads with no posts: {ads_with_no_posts}")

    if mapping:
        logger.debug("Full ad-to-post mapping: %s", dict(mapping))
        print(f"  {ansi.green}Successful mappings:{ansi.reset}")
        for ad_id, post_list in mapping.items():
            if post_list:
                posts_str = ", ".join(post_list)
                print(f"    {ad_id} → [{posts_str}] ({len(post_list)} posts)")
            else:
                print(f"    {ad_id} → {ansi.red}[no posts]{ansi.reset}")

    return mapping


def fetch_all_comments(post_id: str, token: str) -> List[Dict[str, Any]]:
    """Return all comments for a post (depth-1). Returns empty list if permissions insufficient."""
    function_start = time.perf_counter()

    logger.info("Starting fetch_all_comments for post_id: %s", post_id)
    logger.debug("Using token length: %d characters", len(token))

    comments: List[Dict[str, Any]] = []
    params = {
        "fields": "id,from,message,created_time,like_count,user_likes,comment_count,permalink_url",
        "limit": 100,
        "access_token": token,
    }
    endpoint = f"{post_id}/comments"

    logger.debug("Initial params: %s",
                {k: (v[:20] + "..." if k == "access_token" else v) for k, v in params.items()})
    logger.debug("Starting endpoint: %s", endpoint)

    print(f"\n{ansi.blue}DEBUG: Fetching comments for post {post_id}{ansi.reset}")

    page_count = 0
    total_processing_time = 0
    while True:
        # Add rate limiting delay between pages
        page_start = time.perf_counter()
        page_count += 1

        logger.debug("Fetching page %d for post %s", page_count, post_id)

        if page_count > 1:  # Changed from page_count > 0 since we increment before this check
            logger.debug("Rate limiting: waiting %d seconds before page %d",
                        RATE_LIMIT_DELAY, page_count)
            print(f"  {ansi.yellow}Rate limiting: waiting {RATE_LIMIT_DELAY}s "
                  f"before next page...{ansi.reset}")
            time.sleep(RATE_LIMIT_DELAY)

        try:
            logger.debug("Making request for page %d, endpoint: %s", page_count, endpoint)
            data = graph_request(endpoint, params)
            page_time = time.perf_counter() - page_start
            total_processing_time += page_time

            # Debug the response
            if 'error' in data:
                error = data['error']
                logger.error("Comments API error for post %s page %d - Code: %s, Message: %s",
                           post_id, page_count, error.get('code', 'Unknown'),
                           error.get('message', 'Unknown'))

                print(f"  {ansi.red}❌ Comments API Error:{ansi.reset}")
                print(f"    Message: {error.get('message', 'Unknown')}")
                print(f"    Code: {error.get('code', 'Unknown')}")

                # Check for permissions errors specifically
                if error.get('code') in [10, 200, 190] or 'permission' in error.get('message', '').lower():
                    logger.warning("Permissions insufficient for post %s, stopping comment fetch",
                                  post_id)
                    print(f"  {ansi.yellow}⚠️  Permissions insufficient for this post, "
                          f"skipping...{ansi.reset}")
                    break

                logger.error("Other API error for post %s, stopping comment fetch", post_id)
                # Other API errors - also break to avoid infinite loop
                break

            comments_batch = data.get("data", [])
            logger.info("Page %d for post %s: received %d comments in %.3f seconds",
                       page_count, post_id, len(comments_batch), page_time)

            if comments_batch:
                logger.debug("Page %d first comment preview: %s",
                           page_count, comments_batch[0].get('message', 'No message')[:100])

            print(f"  {ansi.green}📥 Received {len(comments_batch)} comments in page {page_count}{ansi.reset}")
            if comments_batch:
                print(f"    First comment preview: {comments_batch[0].get('message', 'No message')[:100]}...")

            comments.extend(comments_batch)
            paging = data.get("paging", {})
            next_url = paging.get("next")

            if not next_url:
                logger.debug("No more pages for post %s, pagination complete", post_id)
                break

            # Parse next_url into new endpoint + params
            parsed = urllib.parse.urlparse(next_url)
            endpoint = parsed.path.lstrip("/")
            params = dict(urllib.parse.parse_qsl(parsed.query))
            logger.debug("Next page URL parsed - endpoint: %s", endpoint)

        except urllib.error.HTTPError as e:
            page_error_time = time.perf_counter() - page_start
            logger.error("HTTP error %d on page %d for post %s after %.3f seconds: %s",
                        e.code, page_count, post_id, page_error_time, str(e))

            # Handle HTTP errors gracefully (like 400 Bad Request for permissions)
            if e.code == 400:
                logger.warning("HTTP 400 (Bad Request) for post %s - likely permissions issue",
                              post_id)
                print(f"  {ansi.yellow}⚠️  HTTP 400 (Bad Request) - likely permissions issue, "
                      f"skipping post...{ansi.reset}")
                break
            if e.code == 403:
                logger.warning("HTTP 403 (Forbidden) for post %s - access denied", post_id)
                print(f"  {ansi.yellow}⚠️  HTTP 403 (Forbidden) - access denied, "
                      f"skipping post...{ansi.reset}")
                break

            logger.error("HTTP error %d for post %s - re-raising to trigger retry logic",
                        e.code, post_id)
            # For other HTTP errors, re-raise to trigger retry logic
            raise
        except (urllib.error.URLError, OSError, ConnectionError, json.JSONDecodeError, KeyError) as e:
            page_error_time = time.perf_counter() - page_start
            logger.error("Unexpected error on page %d for post %s after %.3f seconds: %s",
                        page_count, post_id, page_error_time, str(e))
            print(f"  {ansi.red}❌ Unexpected error fetching comments: {str(e)}{ansi.reset}")
            break

    function_time = time.perf_counter() - function_start
    logger.info("fetch_all_comments completed for post %s: %d comments across %d pages "
               "in %.3f seconds (%.3f seconds processing)",
               post_id, len(comments), page_count, function_time, total_processing_time)

    if comments:
        first_created = comments[0].get('created_time', 'unknown') if comments else 'N/A'
        last_created = comments[-1].get('created_time', 'unknown') if comments else 'N/A'
        logger.debug("Comments summary for post %s: first comment created at %s, last comment created at %s",
                   post_id, first_created, last_created)

    print(f"  {ansi.cyan}📊 Total comments fetched: {len(comments)} across {page_count} pages{ansi.reset}")
    return comments


def resolve_page_token(post_id: str, page_tokens: Dict[str, str], fallback_token: str) -> str:
    """Return a page token suitable for the post. Uses fallback if page not found."""
    logger.debug("Resolving token for post_id: %s", post_id)
    logger.debug("Available page tokens: %d pages", len(page_tokens))

    # post_id may be "{pageId}_{postId}" or just numeric. Extract page prefix if present.
    if "_" in post_id:
        page_id, _ = post_id.split("_", 1)
        logger.debug("Extracted page_id: %s from post_id: %s", page_id, post_id)

        if page_id in page_tokens:
            logger.debug("Found specific page token for page_id: %s", page_id)
            return page_tokens[page_id]

        logger.debug("No specific page token found for page_id: %s, using fallback", page_id)
        return fallback_token

    # If format without underscore, we don't know page id – fall back.
    logger.debug("Post_id has no underscore format, using fallback token")
    return fallback_token


def test_direct_post_access(post_id: str, token: str) -> bool:
    """Test if we can directly access a post ID to debug access issues."""
    print(f"\n{ansi.cyan}DEBUG: Testing direct access to post ID: {post_id}{ansi.reset}")

    try:
        params = {
            "fields": "id,message,created_time,from",
            "access_token": token,
        }
        data = graph_request(f"{post_id}", params)

        print(f"  {ansi.green}✓ Post accessible{ansi.reset}")
        print(f"  Post data: {json.dumps(data, indent=2)}")
        return True

    except (urllib.error.HTTPError, urllib.error.URLError, OSError,
            json.JSONDecodeError, KeyError) as e:
        print(f"  {ansi.red}✗ Post not accessible: {str(e)}{ansi.reset}")
        return False


def main(argv: Optional[List[str]] = None):
    """Main function to orchestrate Facebook ad comments extraction process."""
    script_start = time.perf_counter()

    logger.info("=== Facebook Comments Extraction Script Started ===")
    logger.info("Script arguments: %s", argv)

    description = "Fetch comments for a list of Facebook Ad IDs → Post IDs → Comments"
    parser = argparse.ArgumentParser(description=description)
    ids_help = "Text file containing one Ad ID per line"
    parser.add_argument("--ids-file", type=Path, default=DEFAULT_IDS_FILE, help=ids_help)
    output_help = "Directory to write comments-*.json"
    parser.add_argument("--output", type=Path, default=DATA_FACEBOOK, help=output_help)
    parser.add_argument("--test-post", type=str, help="Test direct access to a specific post ID")
    args = parser.parse_args(argv)

    logger.info("Parsed arguments - ids_file: %s, output: %s, test_post: %s",
               args.ids_file, args.output, args.test_post)

    user_token, page_tokens = load_latest_tokens()

    # Test direct post access if requested
    if args.test_post:
        logger.info("Test mode: testing direct post access for post_id: %s", args.test_post)
        print(f"🧪 Testing direct post access for: {ansi.yellow}{args.test_post}{ansi.reset}")

        # Try with user token
        logger.info("Testing post access with user token")
        print(f"\n{ansi.blue}Testing with user token:{ansi.reset}")
        user_test_result = test_direct_post_access(args.test_post, user_token)
        logger.info("User token test result: %s", user_test_result)

        # Try with page token if available
        if page_tokens:
            page_id, page_token = next(iter(page_tokens.items()))
            logger.info("Testing post access with page token (page_id: %s)", page_id)
            print(f"\n{ansi.blue}Testing with page token (Page ID: {page_id}):{ansi.reset}")
            page_test_result = test_direct_post_access(args.test_post, page_token)
            logger.info("Page token test result: %s", page_test_result)

        logger.info("Test mode completed, exiting")
        return

    logger.info("Starting main processing workflow")

    if not args.ids_file.exists():
        logger.error("IDs file not found: %s", args.ids_file)
        print(f"{ansi.red}IDs file {args.ids_file} not found{ansi.reset}")
        sys.exit(1)

    logger.info("Reading ad IDs from file: %s", args.ids_file)
    ad_ids = [line.strip() for line in args.ids_file.read_text().splitlines() if line.strip()]
    logger.info("Loaded %d ad IDs from file", len(ad_ids))
    logger.debug("Ad IDs (first 10): %s", ad_ids[:10])

    if not ad_ids:
        logger.error("No valid ad IDs found in file")
        print("No Ad IDs provided.")
        sys.exit(1)

    logger.info("Starting ad processing for %d ad IDs", len(ad_ids))
    print(f"🛰️  Processing {ansi.yellow}{len(ad_ids)}{ansi.reset} Ad IDs…")

    # 1. Ads → Posts
    logger.info("=== Phase 1: Resolving Ads to Posts ===")
    phase1_start = time.perf_counter()
    ad_to_posts = ad_ids_to_post_ids(ad_ids, user_token)
    phase1_time = time.perf_counter() - phase1_start
    logger.info("Phase 1 completed in %.3f seconds", phase1_time)

    # Flatten all post IDs while preserving uniqueness
    logger.info("Flattening and deduplicating post IDs")
    all_post_ids = []
    ad_to_post_mapping = {}  # Keep flat mapping for output compatibility

    for ad_id, post_list in ad_to_posts.items():
        for post_id in post_list:
            if post_id not in all_post_ids:
                all_post_ids.append(post_id)
            # For output compatibility, map ad to first post (most common case)
            if ad_id not in ad_to_post_mapping and post_list:
                ad_to_post_mapping[ad_id] = post_list[0]

    unique_posts = len(set(all_post_ids))
    logger.info("Post ID deduplication: %d total posts from ads, %d unique posts", len(all_post_ids), unique_posts)

    print(f"🔗 Resolved {ansi.yellow}{len(all_post_ids)}{ansi.reset} unique Post IDs from {len(ad_ids)} ads.")

    # Show ads with multiple posts
    multi_post_ads = {ad_id: posts for ad_id, posts in ad_to_posts.items() if len(posts) > 1}
    logger.info("Multi-post ads analysis: %d ads have multiple posts", len(multi_post_ads))

    if multi_post_ads:
        logger.debug("Multi-post ads details: %s", {k: len(v) for k, v in multi_post_ads.items()})
        print(f"📋 {ansi.magenta}{len(multi_post_ads)}{ansi.reset} ads have multiple posts:")
        for ad_id, posts in multi_post_ads.items():
            print(f"  Ad {ad_id}: {len(posts)} posts → {', '.join(posts)}")

    # 2. Posts → Comments
    logger.info("=== Phase 2: Fetching Comments from Posts ===")
    logger.info("Starting comment extraction for %d unique posts", len(all_post_ids))
    phase2_start = time.perf_counter()

    all_comments: Dict[str, List[Dict[str, Any]]] = {}
    total_comments = 0
    for i, post_id in enumerate(all_post_ids):
        post_start = time.perf_counter()
        logger.info("Processing post %d/%d: %s", i+1, len(all_post_ids), post_id)

        # Add rate limiting delay between posts (except for the first one)
        if i > 0:
            logger.debug("Rate limiting: waiting %d seconds before post %d", RATE_LIMIT_DELAY, i+1)
            print(f"\n{ansi.yellow}Rate limiting: waiting {RATE_LIMIT_DELAY}s before processing next post...{ansi.reset}")
            time.sleep(RATE_LIMIT_DELAY)

        token = resolve_page_token(post_id, page_tokens, user_token)

        # Debug: Show which token is being used
        if "_" in post_id:
            page_id = post_id.split("_", 1)[0]
            if page_id in page_tokens:
                token_type = f"page token (Page ID: {page_id})"
            else:
                token_type = "user token (fallback - page not found)"
        else:
            token_type = "user token (fallback - no page ID in post)"

        # Show which ads this post belongs to
        source_ads = [ad_id for ad_id, posts in ad_to_posts.items() if post_id in posts]
        ads_info = f"from {len(source_ads)} ad(s): {', '.join(source_ads[:3])}" + ("..." if len(source_ads) > 3 else "")

        logger.info("Post %s: using %s, belongs to %d ads: %s",
                   post_id, token_type, len(source_ads), source_ads[:5])

        print(f"🔑 Using {ansi.yellow}{token_type}{ansi.reset} for post {post_id} "
              f"({i+1}/{len(all_post_ids)}) - {ads_info}")

        comments = fetch_all_comments(post_id, token)
        post_time = time.perf_counter() - post_start

        all_comments[post_id] = comments
        total_comments += len(comments)

        logger.info("Post %s: fetched %d comments in %.3f seconds",
                   post_id, len(comments), post_time)

        comment_msg = f"Post {post_id}: fetched {len(comments)} comments"
        print(f"💬 {ansi.cyan}{comment_msg}{ansi.reset}")

    # 3. Persist
    phase2_time = time.perf_counter() - phase2_start
    logger.info("Phase 2 completed in %.3f seconds", phase2_time)
    logger.info("=== Phase 3: Persisting Results ===")
    phase3_start = time.perf_counter()

    timestamp = datetime.now().strftime("%Y-%m-%d-%H%M%S")
    args.output.mkdir(parents=True, exist_ok=True)
    out_file = args.output / f"comments-{timestamp}.json"

    logger.info("Creating output payload")

    # Calculate additional statistics for metadata
    posts_with_comments = len([p for p in all_comments.values() if len(p) > 0])
    posts_without_comments = len(all_post_ids) - posts_with_comments
    total_script_time = time.perf_counter() - script_start

    payload = {
        "metadata": {
            "created_at": datetime.now().isoformat(),
            "script_version": "enhanced_logging",
            "processing_time_seconds": round(total_script_time, 3),
            "phase_times": {
                "ads_to_posts_seconds": round(phase1_time, 3),
                "posts_to_comments_seconds": round(phase2_time, 3)
            },
            "ad_count": len(ad_ids),
            "unique_post_count": len(all_post_ids),
            "posts_with_comments": posts_with_comments,
            "posts_without_comments": posts_without_comments,
            "comment_count": total_comments,
            "ads_with_multiple_posts": len(multi_post_ads),
        },
        "ad_to_posts": ad_to_posts,  # New: full mapping showing multiple posts per ad
        "ad_to_post": ad_to_post_mapping,  # Legacy: flat mapping for compatibility
        "comments": all_comments,
    }

    logger.info("Writing payload to file: %s", out_file)
    logger.info("Payload size: %d total objects, %d comments across %d posts",
               len(payload), total_comments, len(all_post_ids))

    out_file.write_text(json.dumps(payload, indent=2, ensure_ascii=False))
    phase3_time = time.perf_counter() - phase3_start
    total_script_time = time.perf_counter() - script_start

    logger.info("Phase 3 completed in %.3f seconds", phase3_time)
    logger.info("=== Script completed successfully in %.3f seconds ===",
               total_script_time)

    print(f"✅ Saved results to {ansi.cyan}{out_file}{ansi.reset}")

    # Summary statistics
    print(f"\n📊 {ansi.cyan}Summary:{ansi.reset}")
    print(f"  • Processed {len(ad_ids)} ads")
    print(f"  • Found {len(all_post_ids)} unique posts")
    print(f"  • Collected {total_comments} total comments")
    print(f"  • {len(multi_post_ads)} ads had multiple posts")
    print(f"  • {posts_with_comments} posts had comments, "
          f"{posts_without_comments} posts had no comments")
    print(f"  • Total processing time: {total_script_time:.1f} seconds")

    if total_comments > 0:
        avg_comments = total_comments / posts_with_comments
        logger.info("Statistics: average %.1f comments per post with comments", avg_comments)
        print(f"  • Average comments per post with comments: {avg_comments:.1f}")

    logger.info("Final statistics logged - script completed successfully")


if __name__ == "__main__":
    main()
